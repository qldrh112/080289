{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원-핫 인코딩 적용\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "class2 = pd.read_csv('../data/class2.csv')\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "onehot_encoder = preprocessing.OneHotEncoder()\n",
    "\n",
    "train_x = label_encoder.fit_transform(class2['class2'])\n",
    "print(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 13, 'is': 7, 'last': 8, 'chance': 2, 'and': 0, 'if': 6, 'you': 15, 'do': 3, 'not': 10, 'have': 5, 'will': 14, 'never': 9, 'get': 4, 'any': 1, 'one': 11, 'please': 12}\n"
     ]
    }
   ],
   "source": [
    "# 코퍼스에 카운터 벡터 적용\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is last chance.',\n",
    "    'and if you do not have this chance.',\n",
    "    'you will never get any chance.',\n",
    "    'will you do get this one?',\n",
    "    'please, get this chance.',\n",
    "]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "# 학습한 단어 인덱스를 출력\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 배열 변환\n",
    "# 주어진 문장의 단어의 개수를 카운팅한 뒤, 사전에서 단어를 찾고, 있으면 그 인덱스에 개수를 저장\n",
    "vect.transform(['you will never get any chance.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'last': 6, 'chance': 1, 'if': 5, 'you': 11, 'do': 2, 'not': 8, 'have': 4, 'will': 10, 'never': 7, 'get': 3, 'any': 0, 'one': 9}\n"
     ]
    }
   ],
   "source": [
    "# 불용어를 제거한 카운터 벡터\n",
    "# stop_words를 사용하여 is, not, an 같은 불용어를 제거함\n",
    "vect = CountVectorizer(stop_words=['and', 'is', 'please', 'this']).fit(corpus)\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사도를 위한 3 x 3 행렬을 만들었습니다.\n",
      "[[1.       0.224325 0.      ]\n",
      " [0.224325 1.       0.      ]\n",
      " [0.       0.       1.      ]]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF를 적용한 후, 행렬로 표현\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "doc = ['I like machine learning', 'I love deep learning', 'I run everyday']\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(doc)\n",
    "doc_distance = (tfidf_matrix * tfidf_matrix.T)\n",
    "print('유사도를 위한', str(doc_distance.get_shape()[0]), 'x', str(doc_distance.get_shape()[1]), '행렬을 만들었습니다.')\n",
    "print(doc_distance.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['once', 'upon', 'a', 'time', 'in', 'london', ',', 'the', 'darlings', 'went', 'out', 'to', 'a', 'dinner', 'party', 'leaving', 'their', 'three', 'children', 'wendy', ',', 'jhon', ',', 'and', 'michael', 'at', 'home', '.'], ['after', 'wendy', 'had', 'tucked', 'her', 'younger', 'brothers', 'jhon', 'and', 'michael', 'to', 'bed', ',', 'she', 'went', 'to', 'read', 'a', 'book', '.'], ['she', 'heard', 'a', 'boy', 'sobbing', 'outside', 'her', 'window', '.'], ['he', 'was', 'flying', '.'], ['there', 'was', 'little', 'fairy', 'fluttering', 'around', 'him', '.'], ['wendy', 'opened', 'the', 'window', 'to', 'talk', 'to', 'him', '.'], ['“', 'hello', '!'], ['who', 'are', 'you', '?'], ['why', 'are', 'you', 'crying', '”', ',', 'wendy', 'asked', 'him', '.'], ['“', 'my', 'name', 'is', 'peter', 'pan', '.'], ['my', 'shadow', 'wouldn', '’', 't', 'stock', 'to', 'me.', '”', ',', 'he', 'replied', '.'], ['she', 'asked', 'him', 'to', 'come', 'in', '.'], ['peter', 'agreed', 'and', 'came', 'inside', 'the', 'room', '.'], ['wendy', 'took', 'his', 'shadow', 'and', 'sewed', 'it', 'to', 'his', 'shoe', 'tips', '.'], ['now', 'his', 'shadow', 'followed', 'him', 'wherever', 'peter', 'pan', 'went', '!'], ['he', 'was', 'delighted', 'and', 'asked', 'wendy', '“', 'why', 'don', '’', 't', 'you', 'come', 'with', 'me', 'to', 'my', 'home', '.'], ['the', 'neverland', '.'], ['i', 'lived', 'there', 'with', 'my', 'fairy', 'tinker', 'bell.', '”', 'wendy', '?'], ['“', 'oh', '!'], ['what', 'a', 'wonderful', 'idea', '!'], ['let', 'me', 'wake', 'up', 'john', 'and', 'micheal', 'too', '.'], ['could', 'you', 'teach', 'us', 'how', 'to', 'fly', '?', '”', '.'], ['“', 'yes', '!'], ['of', 'course', '!'], ['get', 'them', 'we', 'will', 'all', 'fly', 'together.', '”', 'peter', 'pan', 'replied', 'and', 'so', 'it', 'was', '.'], ['five', 'little', 'figures', 'flew', 'out', 'of', 'the', 'window', 'of', 'the', 'darlings', 'and', 'headed', 'towards', 'neverland', '.'], ['as', 'they', 'flew', 'over', 'the', 'island', ',', 'peter', 'pan', 'told', 'the', 'children', 'more', 'about', 'his', 'homeland', '.'], ['“', 'all', 'the', 'children', 'who', 'get', 'lost', 'come', 'and', 'stay', 'with', 'tinker', 'bell', 'and', 'me', ',', '”', 'peter', 'told', 'them', '.'], ['the', 'indians', 'also', 'live', 'in', 'neverland', '.'], ['the', 'mermaids', 'live', 'in', 'the', 'lagoon', 'around', 'the', 'island', '.'], ['and', 'a', 'very', 'mean', 'pirate', 'called', 'captain', 'hook', 'keeps', 'troubling', 'everyone', '.'], ['“', 'crocodile', 'bit', 'his', 'one', 'arm', '.'], ['so', 'the', 'captain', 'had', 'to', 'put', 'a', 'hook', 'in', 'its', 'place', '.'], ['since', 'then', 'he', 'is', 'afraid', 'of', 'crocodiles', '.'], ['and', 'rightly', 'so', '!'], ['if', 'the', 'crocodile', 'ever', 'found', 'captain', 'hook', 'it', 'will', 'eat', 'up', 'the', 'rest', 'of', 'it', 'couldn', '’', 't', 'eat', 'last', 'time.', '”', 'peter', 'told', 'them', '.'], ['soon', 'they', 'landed', 'on', 'the', 'island', '.'], ['and', 'to', 'the', 'surprise', 'of', 'wendy', ',', 'jhon', 'and', 'michael', ',', 'peter', 'pan', 'let', 'them', 'in', 'through', 'a', 'small', 'opening', 'in', 'a', 'tree', '.'], ['inside', 'the', 'tree', 'was', 'a', 'large', 'room', 'with', 'children', 'inside', 'it', '.'], ['somewhere', 'huddled', 'by', 'the', 'fire', 'in', 'the', 'corner', 'and', 'somewhere', 'playing', 'amongst', 'themselves', '.'], ['their', 'faces', 'lit', 'up', 'when', 'they', 'saw', 'peter', 'pan', ',', 'tinker', 'bell', ',', 'and', 'their', 'guests', '.'], ['“', 'hello', 'everyone', '.'], ['this', 'is', 'wendy', ',', 'jhon', ',', 'and', 'michael', '.'], ['they', 'will', 'be', 'staying', 'with', 'us', 'from', 'now', 'on.', '”', 'peter', 'pan', 'introduced', 'them', 'to', 'all', 'children', '.'], ['children', 'welcomed', 'wendy', ',', 'jhon', ',', 'and', 'michael', '.'], ['a', 'few', 'days', 'passed', '.'], ['and', 'they', 'settled', 'into', 'a', 'routine', '.'], ['wendy', 'would', 'take', 'care', 'of', 'all', 'the', 'children', 'in', 'the', 'day', 'and', 'would', 'go', 'out', 'with', 'peter', 'pan', 'and', 'her', 'brothers', 'in', 'the', 'evening', 'to', 'learn', 'about', 'the', 'island', '.'], ['she', 'would', 'cook', 'for', 'them', 'and', 'stitch', 'new', 'clothes', 'for', 'them', '.'], ['he', 'even', 'made', 'a', 'lovely', 'new', 'dress', 'for', 'tinker', 'bell', '.'], ['one', 'evening', ',', 'as', 'they', 'were', 'out', 'exploring', 'the', 'island', 'peter', 'pan', 'warned', 'everyone', 'and', 'said', ',', '“', 'hide', '!'], ['hide', '!'], ['pirates', '!'], ['and', 'they', 'have', 'kidnapped', 'the', 'indian', 'princess', 'tiger', 'lily', '.'], ['they', 'have', 'kept', 'her', 'there', ',', 'tied', 'up', 'by', 'the', 'rocks', ',', 'near', 'the', 'water.', '”', 'peter', 'was', 'afraid', 'and', 'the', 'princess', 'would', 'drown', ',', 'is', 'she', 'fell', 'into', 'the', 'water', '.'], ['so', ',', 'in', 'a', 'voice', 'that', 'sounded', 'like', 'captain', 'hook', ',', 'he', 'shouted', 'instructions', 'to', 'the', 'pirates', 'who', 'guarded', 'her', ',', '“', 'you', 'fools', '!'], ['let', 'her', 'go', 'at', 'once', '!'], ['do', 'it', 'before', 'i', 'come', 'there', 'or', 'else', 'i', 'will', 'throw', 'each', 'one', 'of', 'you', 'into', 'the', 'water.', '”', 'the', 'pirates', 'got', 'scared', 'and', 'immediately', 'released', 'the', 'princes', '.'], ['she', 'quickly', 'dived', 'into', 'the', 'water', 'and', 'swam', 'to', 'the', 'safety', 'of', 'her', 'home', '.'], ['soon', 'everyone', 'found', 'out', 'how', 'peter', 'pan', 'had', 'rescued', 'the', 'princess', '.'], ['when', 'captain', 'hook', 'found', 'out', 'how', 'peter', 'had', 'tricked', 'his', 'men', 'he', 'was', 'furious', '.'], ['and', 'swore', 'to', 'have', 'his', 'revenge', '.'], ['that', 'night', 'wendy', 'told', 'peter', 'pan', ',', 'that', 'she', 'and', 'her', 'brother', 'wanted', 'to', 'go', 'back', 'home', 'since', 'they', 'missed', 'their', 'parents', '.'], ['she', 'said', 'if', 'the', 'lost', 'children', 'could', 'also', 'return', 'to', 'her', 'world', 'they', 'could', 'find', 'a', 'nice', 'home', 'for', 'them', '.'], ['peter', 'pan', 'didn', '’', 't', 'want', 'to', 'leave', 'neverland', '.'], ['but', 'the', 'sake', 'of', 'the', 'lost', 'children', 'he', 'agreed', ',', 'although', 'a', 'bit', 'sadly', '.'], ['he', 'would', 'miss', 'his', 'friends', 'dearly', '.'], ['the', 'next', 'morning', 'all', 'the', 'lost', 'children', 'left', 'with', 'wendy', ',', 'jhon', ',', 'and', 'michael', '.'], ['but', 'on', 'the', 'way', ',', 'captain', 'hook', 'and', 'his', 'men', 'kidnapped', 'all', 'of', 'them', '.'], ['he', 'tied', 'them', 'and', 'kept', 'them', 'on', 'once', 'of', 'his', 'ships', '.'], ['as', 'soon', 'as', 'peter', 'found', 'out', 'about', 'it', 'he', 'rushed', 'to', 'the', 'ship', '.'], ['he', 'swung', 'himself', 'from', 'a', 'tress', 'branch', 'and', 'on', 'to', 'the', 'deck', 'of', 'the', 'ship', 'where', 'all', 'the', 'children', 'were', 'tied', 'up', '.'], ['he', 'swung', 'his', 'sword', 'bravely', 'and', 'threw', 'over', 'the', 'pirates', 'who', 'tried', 'to', 'stop', 'him', '.'], ['quickly', 'he', 'released', 'everyone', 'from', 'their', 'captor', '’', 's', 'ties', '.'], ['wendy', ',', 'jhon', ',', 'michael', 'and', 'tinker', 'bell', 'helped', 'all', 'the', 'children', 'into', 'the', 'water', ',', 'where', 'their', 'friends', 'from', 'the', 'indian', 'camp', 'were', 'ready', 'with', 'smaller', 'boats', 'to', 'take', 'them', 'to', 'safety', 'peter', 'pan', 'now', 'went', 'looking', 'for', 'captain', 'hook', '.'], ['“', 'let', 'us', 'finished', 'this', 'forever', 'mr.', 'hook', '”', ',', 'peter', 'challenged', 'captain', 'hook', '.'], ['“', 'yes', '!'], ['peter', 'pan', ',', 'you', 'have', 'caused', 'me', 'enough', 'trouble', '.'], ['it', 'is', 'time', 'that', 'we', 'finished', 'this.', '”', 'hook', 'replied', '.'], ['with', 'his', 'sword', 'drawn', ',', 'he', 'raced', 'towards', 'peter', 'pan', '.'], ['quick', 'on', 'his', 'feet', ',', 'peter', 'pan', 'stepped', 'aside', 'and', 'pushed', 'hook', 'inside', 'the', 'sea', 'where', 'the', 'crocodile', 'was', 'waiting', 'to', 'eat', 'the', 'rest', 'of', 'hook', '.'], ['everyone', 'rejoiced', 'as', 'captain', 'hook', 'was', 'out', 'of', 'their', 'lives', 'forever', '.'], ['everybody', 'headed', 'back', 'to', 'london', '.'], ['mr.', 'and', 'mrs', '.'], ['darling', 'was', 'so', 'happy', 'to', 'see', 'their', 'children', 'and', 'they', 'agreed', 'to', 'adopt', 'the', 'lost', 'children', '.'], ['they', 'even', 'asked', 'peter', 'pan', 'to', 'come', 'and', 'live', 'with', 'them', '.'], ['but', 'peter', 'pan', 'said', ',', 'he', 'never', 'wanted', 'to', 'grow', 'up', ',', 'so', 'he', 'and', 'tinker', 'bell', 'will', 'go', 'back', 'to', 'neverland', '.'], ['peter', 'pan', 'promised', 'everyone', 'that', 'he', 'will', 'visit', 'again', 'sometime', '!'], ['and', 'he', 'flew', 'out', 'of', 'the', 'window', 'with', 'tinker', 'bell', 'by', 'his', 'side', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋을 메모리로 로딩하고 토큰화 적용\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sample = open('../data/peter.txt', 'r', encoding='UTF8')\n",
    "s = sample.read()\n",
    "# 줄 바꿈을 공백으로 변환\n",
    "f = s.replace('\\n', ' ')\n",
    "data = []\n",
    "\n",
    "# 로딩한 파일의 각 문장마다 반복\n",
    "for i in sent_tokenize(f):\n",
    "    temp = []\n",
    "    # 문장을 단어로 토큰화\n",
    "    for j in word_tokenize(i):\n",
    "        # 토큰화된 단어를 소문자로 변환하여 temp에 저장\n",
    "        temp.append(j.lower())\n",
    "    data.append(temp)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'peter' 'wendy' - CBOW :  0.074393824\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋에 CBOW 적용 후 'peter'와 'wendy'의 유사성 확인\n",
    "model1 = gensim.models.Word2Vec(data, min_count=1, vector_size=100, window=5, sg=0)\n",
    "# 결과 출력\n",
    "print(\"Cosine similarity between 'peter' \" + \"'wendy' - CBOW : \", model1.wv.similarity('peter', 'wendy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'peter' 'hook' - CBOW :  0.027709836\n"
     ]
    }
   ],
   "source": [
    "# 'peter'와 'hook'의 유사성 확인\n",
    "print(\"Cosine similarity between 'peter' \" + \"'hook' - CBOW : \", model1.wv.similarity('peter', 'hook'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'peter' 'wendy' - Skip Gram :  0.40088683\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋에 skip-gram 적용 후 'peter'와 'wendy'의 유사성 확인\n",
    "# skip-gram 모델 사용\n",
    "model2 = gensim.models.Word2Vec(data, min_count=1, vector_size=100, window=5, sg=1)\n",
    "# 결과 출력\n",
    "print(\"Cosine similarity between 'peter' \" + \"'wendy' - Skip Gram : \", model2.wv.similarity('peter', 'wendy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'peter' 'hook' - Skip Gram :  0.5201673\n"
     ]
    }
   ],
   "source": [
    "# 'peter'와 'hook'의 유사성\n",
    "# peter와 hook이 의미적으로 얼마나 가까운지 평가하는 것, 유사성을 판단\n",
    "print(\"Cosine similarity between 'peter' \" + \"'hook' - Skip Gram : \", model2.wv.similarity('peter', 'hook'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 및 데이터 호출\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import FastText\n",
    "\n",
    "model = FastText('../data/peter.txt', vector_size=4, window=3, min_count=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4592452\n"
     ]
    }
   ],
   "source": [
    "# 'peter', 'wendy'에 대한 코사인 유사도\n",
    "sim_score = model.wv.similarity('peter', 'wendy')\n",
    "print(sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.043825686\n"
     ]
    }
   ],
   "source": [
    "# 'peter'와 'hook'에 대한 코사인 유사도\n",
    "sim_score = model.wv.similarity('peter', 'hook')\n",
    "print(sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리와 사전 훈련된 모델 호출\n",
    "from __future__ import print_function\n",
    "# gensim은 자연어를 벡터로 변환하는 데 필요한 편의 기능을 제공하는 라이브러리\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_kr = KeyedVectors.load_word2vec_format('../data/wiki.ko.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 박도경, Similar: 83.59906673431396%\n",
      "Word: 이도경, Similar: 82.49361515045166%\n",
      "Word: 권도경, Similar: 80.06789088249207%\n",
      "Word: 도경수, Similar: 79.98784184455872%\n",
      "Word: 도경리, Similar: 78.66216897964478%\n",
      "Word: 하경, Similar: 78.43520641326904%\n",
      "Word: 최호경, Similar: 77.53516435623169%\n",
      "Word: 송욱경, Similar: 77.4889349937439%\n",
      "Word: 강문경, Similar: 77.4862289428711%\n",
      "Word: 옥도경, Similar: 77.2731363773346%\n"
     ]
    }
   ],
   "source": [
    "# '도경'과 유사한 단어와 유사도 확인\n",
    "find_similar_to = '도경'\n",
    "\n",
    "for similar_word in model_kr.similar_by_word(find_similar_to):\n",
    "    print(f'Word: {similar_word[0]}, Similar: {similar_word[1] * 100}%')\n",
    "\n",
    "# dir(model_kr)\n",
    "# model_kr.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('초식동물', 0.7804122567176819), ('거대동물', 0.7547270655632019), ('육식동물의', 0.7547166347503662), ('유두동물', 0.753511369228363), ('반추동물', 0.7470757961273193), ('독동물', 0.7466291785240173), ('육상동물', 0.746031641960144), ('유즐동물', 0.7450903654098511), ('극피동물', 0.7449344396591187), ('복모동물', 0.742434561252594)]\n"
     ]
    }
   ],
   "source": [
    "# '동물', '육식동물'에는 긍정적이지만, '사람'에는 부정적인 단어와 유사도 확인\n",
    "similarities = model_kr.most_similar(positive=['동물', '육식동물'], negative=['사람'])\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라이브러리 호출 및 데이터셋 로딩\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = datapath('C:/projects/pytorch/080289/chap10/data/glove.6B.100d.txt')\n",
    "# 글로브 데이터를 워드투벡터 형태로 변환\n",
    "word2vec_glove_file = get_tmpfile('glove.6B.100d.word2vec.txt')\n",
    "print(glove2word2vec(glove_file, word2vec_glove_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('legislation', 0.8072139620780945), ('proposal', 0.730686366558075), ('senate', 0.7142541408538818), ('bills', 0.704440176486969), ('measure', 0.6958035230636597), ('passed', 0.690624475479126), ('amendment', 0.6846879720687866), ('provision', 0.6845567226409912), ('plan', 0.6816462874412537), ('clinton', 0.6663140058517456)]\n"
     ]
    }
   ],
   "source": [
    "# 'bill'과 유사한 단어의 리스트를 반환\n",
    "# load_word2_vec_format() 메서드를 이용하여 word2vec.c 형식으로 벡터를 가져옵니다.\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
    "# 단어(bill)을 기준으로 가장 유사한 단어의 리스트를 보여 줍니다.\n",
    "print(model.most_similar('bill'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('peach', 0.688809871673584),\n",
       " ('mango', 0.683819055557251),\n",
       " ('plum', 0.6684104204177856),\n",
       " ('berry', 0.6590359210968018),\n",
       " ('grove', 0.658155083656311),\n",
       " ('blossom', 0.6503506302833557),\n",
       " ('raspberry', 0.6477391719818115),\n",
       " ('strawberry', 0.6442098021507263),\n",
       " ('pine', 0.6390928626060486),\n",
       " ('almond', 0.6379212737083435)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'cherry'와 유사한 단어의 리스트를 반환\n",
    "# 단어(cherry) 기준으로 가장 유사한 단어의 리스트를 보여 줌\n",
    "model.most_similar('cherry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kazushige', 0.48343509435653687),\n",
       " ('askerov', 0.4778185784816742),\n",
       " ('lakpa', 0.46915265917778015),\n",
       " ('ex-gay', 0.4571332633495331),\n",
       " ('tadayoshi', 0.4522107243537903),\n",
       " ('turani', 0.44810065627098083),\n",
       " ('saglam', 0.4469599425792694),\n",
       " ('aijun', 0.4435270130634308),\n",
       " ('adjustors', 0.44235292077064514),\n",
       " ('nyum', 0.4423118233680725)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'cherry'와 관련성이 없는 단어의 리스트를 반환\n",
    "# 단어와 관련성이 없는 단어를 반환\n",
    "model.most_similar(negative=['cherry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen: 0.7699\n"
     ]
    }
   ],
   "source": [
    "# 'woman', 'king'과 유사성이 높으면서 'man'과 관련성이 없는 단어를 반환\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print(f'{result[0][0]}: {result[0][1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'champagne'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'australia' 'beer', 'france'와 관련성이 있는 단어를 변환\n",
    "def analogy(x1, x2, y1):\n",
    "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
    "    return result[0][0]\n",
    "analogy('australia', 'beer', 'france')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longest\n"
     ]
    }
   ],
   "source": [
    "# 'tail', 'tallest', 'long' 단어를 기반으로 새로운 단어를 유추\n",
    "print(analogy('tall', 'tallest', 'long'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cereal\n"
     ]
    }
   ],
   "source": [
    "# 'breakfast cereal dinner lunch' 중 유사도가 낮은 단어를 반환\n",
    "# 유사도가 가장 낮은 단어를 반환\n",
    "print(model.doesnt_match('breakfast cereal dinner lunch'.split()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
